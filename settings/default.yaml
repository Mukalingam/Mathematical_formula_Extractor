# Input/Output/Name
data: "dataset/data/train.pkl"
valdata: "dataset/data/val.pkl"
tokenizer: "dataset/data/tokenizer.json"
output_path: "outputs"
model_path: "checkpoints"
load_chkpt: null # continue training
name: "pix2tex"

# Training parameters
epochs: 10
batchsize: 8

# Testing parameters
testbatchsize: 20
valbatches: 100
temperature: 0.2

# Optimizer configurations
optimizer: "Adam"
scheduler: "OneCycleLR"
lr: 0.001
max_lr: 0.005
betas: [0.9, 0.999]

# Parameters for model architectures
max_width: 672
max_height: 192
min_width: 96
min_height: 32
channels: 1
patch_size: 16
# Encoder / Decoder
dim: 256
backbone_layers: [3,4,9]
encoder_depth: 4
num_layers: 4
decoder_args: 
  cross_attend: true
  ff_glu: false
  attn_on_attn: false
  use_scalenorm: false
  rel_pos_bias: false
heads: 8
num_tokens: 8000
max_seq_len: 512

# Other
seed: 42
id: null
sample_freq: 3000
test_samples: 5
save_freq: 5 # save every nth epoch
debug: False
pad: False

# Token ids
pad_token: 0
bos_token: 1
eos_token: 2